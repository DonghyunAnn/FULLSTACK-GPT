{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs and Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-davinci-003\n",
    "from langchain.llms.openai import OpenAI\n",
    "# gpt-r.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# predict just one string, message\n",
    "a = llm.predict(\"How many planets are there\")\n",
    "b = chat.predict(\"How many planets are there\")\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict list of messages\n",
    "# message constructors\n",
    "# 맥락 및 조건 설정 가능, 작성한 것들 memory에 추가, \n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content = \"You are a georgraphy expoert. And you only reply in Inalian.\",\n",
    "    ),\n",
    "    AIMessage(content= \"Ciao, mi chiamo Paolo\"),\n",
    "    HumanMessage(\n",
    "        content = \"What is the distance between Mexico and Thailand. Also, what isyour name?\",\n",
    "    ),\n",
    "]\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 템플릿은 일종의 validation 기능을 해서 쓸대없는 문장을 소비하지 않게 만들어줌\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"What is the distance between {country_a} and {country_b}.\"\n",
    ")\n",
    "prompt = template.format(country_a = \"Mexico\",country_b = \"Thailand\")\n",
    "chat.predict(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"You are a georgraphy expoert. And you only reply in {language}.\"),\n",
    "        (\"ai\",\"Ciao, mi chiamo {name}\"),\n",
    "        (\"human\",\"What is the distance between {country_a} and {country_b}. Also, what is your name?\")\n",
    "    ]\n",
    ")\n",
    "prompt = template.format_messages(\n",
    "    language = 'Greek',\n",
    "    name = \"Socrates\",\n",
    "    country_a = \"Mexico\",\n",
    "    country_b = \"Thailand\"\n",
    "\n",
    ")\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser and LangChainExpressionLanguage(LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Expression Language\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# LLM 응답을 변형해야 할 때 사용하기 위해서\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self,text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip,items))\n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello,how, are, you\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a list generating machine. Everything you are asked will be answered with a comma seperated list of max {max_items}. Do NOT reply with anything else\"),\n",
    "    (\"human\",\"{question}\")\n",
    "]\n",
    ")\n",
    "prompt = template.format_messages(max_items = 10,question = \"What are the planets?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format_messages(max_items = 10, question = \"What are the colors\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 체인을 이용하면 각각의 predict 결과를 연결해서 사용할 수 있음\n",
    "- R의 chain 같은 느낌\n",
    "```\n",
    "chain1 = template1 | chat | outputparser1\n",
    "chain2 = template2 | chat | outputparser2\n",
    "chain_sum = chain1 | chain2 | outputparser_sum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드가 너무 김\n",
    "# 기존 작업을 위해 작성한 코드: Chat model, 파서 생성, 템플릿 생성, message format, predict, 파서 호출, 파싱 \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a list generating machine. Everything you are asked will be answered with a comma seperated list of max {max_items}. Do NOT reply with anything else\"),\n",
    "    (\"human\",\"{question}\")\n",
    "]\n",
    ")\n",
    "# chain 사용\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the poketmons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining Chains\n",
    "- [invoke 설명](https://python.langchain.com/docs/expression_language/interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# Streaming은 모델의 응답이 생성되는걸 보게 해줌, \n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,streaming=True,callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "chef_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a world-class international chef. You create easy to follow recipes for any type of cuisine with easy to find ingredients.\"),\n",
    "    (\"human\",\"I want to cook {cuisine} food\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_template | chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a vegetereian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "    \"human\",\"{recipe}\"\n",
    "])\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\":\"indian\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module format](https://python.langchain.com/docs/modules/)\n",
    "- Model I/O\n",
    "    - Model Input & Output\n",
    "    - prompts, language model, output parser\n",
    "\n",
    "\n",
    "- Retrieval\n",
    "    - 외부데이터를 모델에 어떻게 적용하느냐\n",
    "\n",
    "- Chains\n",
    "\n",
    "- Memory\n",
    "\n",
    "- Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fewshot Prompt Template\n",
    "- prompt template을 디스크에 저장하고 로드할 수 있기 때문에 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "t = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "t.format(country = \"France\")\n",
    "\n",
    "t= PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "t.format(country = \"France\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "example_prompt =  PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "# suffix 는 사용자의 질문\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    examples = examples,\n",
    "    suffix = \"Human: What do you know about {country}?\",\n",
    "    input_variables = [\"country\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# chat.predict(prompt.format(country = 'Germany'))\n",
    "chain = prompt | chat\n",
    "chain.invoke({\n",
    "    \"country\": \"Turkey\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Chat Message Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# example prompt는 example과 key값이 일치해야함\n",
    "example_prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\",\"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    examples = examples,\n",
    "    )\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\" You are a geography expert, you gives short answers\"),\n",
    "    example_prompt,\n",
    "    (\"human\",\"What do you know about {country}?\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\n",
    "    \"country\": \"THailand\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_prompt.format(country = \"Germany\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length Based Example Selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:What do you know about Greece?\\nAI:\\n        I know this:\\n        Capital: Athens\\n        Language: Greek\\n        Food: Souvlaki and Feta Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self,examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "    \n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "    \n",
    "example_prompt =  PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples = examples,\n",
    "#     example_prompt = example_prompt,\n",
    "#     max_length = 80)\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    example_selector= example_selector,\n",
    "    suffix = \"Human: What do you know about {country}?\",\n",
    "    input_variables = [\"country\"],\n",
    "    )\n",
    "\n",
    "prompt.format(country='Brazil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize\n",
    "- 디스크에서 프롬프트를 가져오는 법\n",
    "- 이를 통해 prompt를 별도로 저장 가능함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country = \"Germany\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming= True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt = final,\n",
    "    pipeline_prompts= )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
