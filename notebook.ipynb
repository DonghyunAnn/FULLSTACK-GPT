{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs and Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-davinci-003\n",
    "from langchain.llms.openai import OpenAI\n",
    "# gpt-r.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# predict just one string, message\n",
    "a = llm.predict(\"How many planets are there\")\n",
    "b = chat.predict(\"How many planets are there\")\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict list of messages\n",
    "# message constructors\n",
    "# 맥락 및 조건 설정 가능, 작성한 것들 memory에 추가, \n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content = \"You are a georgraphy expoert. And you only reply in Inalian.\",\n",
    "    ),\n",
    "    AIMessage(content= \"Ciao, mi chiamo Paolo\"),\n",
    "    HumanMessage(\n",
    "        content = \"What is the distance between Mexico and Thailand. Also, what isyour name?\",\n",
    "    ),\n",
    "]\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 템플릿은 일종의 validation 기능을 해서 쓸대없는 문장을 소비하지 않게 만들어줌\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"What is the distance between {country_a} and {country_b}.\"\n",
    ")\n",
    "prompt = template.format(country_a = \"Mexico\",country_b = \"Thailand\")\n",
    "chat.predict(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"You are a georgraphy expoert. And you only reply in {language}.\"),\n",
    "        (\"ai\",\"Ciao, mi chiamo {name}\"),\n",
    "        (\"human\",\"What is the distance between {country_a} and {country_b}. Also, what is your name?\")\n",
    "    ]\n",
    ")\n",
    "prompt = template.format_messages(\n",
    "    language = 'Greek',\n",
    "    name = \"Socrates\",\n",
    "    country_a = \"Mexico\",\n",
    "    country_b = \"Thailand\"\n",
    "\n",
    ")\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser and LangChainExpressionLanguage(LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Expression Language\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# LLM 응답을 변형해야 할 때 사용하기 위해서\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self,text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip,items))\n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello,how, are, you\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a list generating machine. Everything you are asked will be answered with a comma seperated list of max {max_items}. Do NOT reply with anything else\"),\n",
    "    (\"human\",\"{question}\")\n",
    "]\n",
    ")\n",
    "prompt = template.format_messages(max_items = 10,question = \"What are the planets?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format_messages(max_items = 10, question = \"What are the colors\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 체인을 이용하면 각각의 predict 결과를 연결해서 사용할 수 있음\n",
    "- R의 chain 같은 느낌\n",
    "```\n",
    "chain1 = template1 | chat | outputparser1\n",
    "chain2 = template2 | chat | outputparser2\n",
    "chain_sum = chain1 | chain2 | outputparser_sum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드가 너무 김\n",
    "# 기존 작업을 위해 작성한 코드: Chat model, 파서 생성, 템플릿 생성, message format, predict, 파서 호출, 파싱 \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are a list generating machine. Everything you are asked will be answered with a comma seperated list of max {max_items}. Do NOT reply with anything else\"),\n",
    "    (\"human\",\"{question}\")\n",
    "]\n",
    ")\n",
    "# chain 사용\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the poketmons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining Chains\n",
    "- [invoke 설명](https://python.langchain.com/docs/expression_language/interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# Streaming은 모델의 응답이 생성되는걸 보게 해줌, \n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,streaming=True,callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "chef_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a world-class international chef. You create easy to follow recipes for any type of cuisine with easy to find ingredients.\"),\n",
    "    (\"human\",\"I want to cook {cuisine} food\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_template | chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a vegetereian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "    \"human\",\"{recipe}\"\n",
    "])\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\":\"indian\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module format](https://python.langchain.com/docs/modules/)\n",
    "- Model I/O\n",
    "    - Model Input & Output\n",
    "    - prompts, language model, output parser\n",
    "\n",
    "\n",
    "- Retrieval\n",
    "    - 외부데이터를 모델에 어떻게 적용하느냐\n",
    "\n",
    "- Chains\n",
    "\n",
    "- Memory\n",
    "\n",
    "- Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fewshot Prompt Template\n",
    "- prompt template을 디스크에 저장하고 로드할 수 있기 때문에 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "t = PromptTemplate(\n",
    "    template = \"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "t.format(country = \"France\")\n",
    "\n",
    "t= PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "t.format(country = \"France\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "example_prompt =  PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "# suffix 는 사용자의 질문\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    examples = examples,\n",
    "    suffix = \"Human: What do you know about {country}?\",\n",
    "    input_variables = [\"country\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# chat.predict(prompt.format(country = 'Germany'))\n",
    "chain = prompt | chat\n",
    "chain.invoke({\n",
    "    \"country\": \"Turkey\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Chat Message Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# example prompt는 example과 key값이 일치해야함\n",
    "example_prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\",\"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    examples = examples,\n",
    "    )\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\" You are a geography expert, you gives short answers\"),\n",
    "    example_prompt,\n",
    "    (\"human\",\"What do you know about {country}?\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\n",
    "    \"country\": \"THailand\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_prompt.format(country = \"Germany\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length Based Example Selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming = True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self,examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "    \n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "    \n",
    "example_prompt =  PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples = examples,\n",
    "#     example_prompt = example_prompt,\n",
    "#     max_length = 80)\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    example_selector= example_selector,\n",
    "    suffix = \"Human: What do you know about {country}?\",\n",
    "    input_variables = [\"country\"],\n",
    "    )\n",
    "\n",
    "prompt.format(country='Brazil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize\n",
    "- 디스크에서 프롬프트를 가져오는 법\n",
    "- 이를 통해 prompt를 별도로 저장 가능함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country = \"Germany\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    streaming= True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt = final,\n",
    "    pipeline_prompts= prompts\n",
    ")\n",
    "\n",
    "full_prompt.format(\n",
    "    character = \"Pirate\",\n",
    "    example_question = \"What is your location\",\n",
    "    example_answer = \"Arggg! That is a secret! Arg arg!!\",\n",
    "    question = \"What is your fav food?\"\n",
    ")\n",
    "\n",
    "chain  =  full_prompt | chat\n",
    "chain.invoke({\n",
    "    \"character\" : \"Pirate\",\n",
    "    \"example_question\" : \"What is your location\",\n",
    "    \"example_answer\" : \"Arggg! That is a secret! Arg arg!!\",\n",
    "    \"question\" : \"What is your fav food?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "- LM의 응답을 저장할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "# 자동으로 db를 만들며 저장함 third party를 통해 다양한 방법 가능\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True) # 세부 정보 다 볼 수 있음\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    # streaming= True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ]\n",
    ")\n",
    "chat.predict(\"How do you make Italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속도가 달라지는 것을 볼 수 있음\n",
    "chat.predict(\"How do you make Italian pasta?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize and Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "chat = ChatOpenAI(\n",
    "    temperature= 0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"whar is the recipe for soju\")\n",
    "    b = chat.predict(\"whar is the recipe for bread\")\n",
    "    print(a,b,\"\\n\")\n",
    "    print(usage)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "chat = OpenAI(\n",
    "    temperature=0.1,\n",
    "    max_tokens=450,\n",
    "    model = \"gpt-3.5-turbo\"\n",
    ")\n",
    "# 특별하게 세팅한 모델 저장 & 호출 가능\n",
    "chat.save(\"model.json\")\n",
    "\n",
    "call_chat = load_llm(\"model.json\")\n",
    "call_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "- 챗봇의 기본\n",
    "- openai api는 메모리 기능을 포함하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer Memory\n",
    "- 이전 모든 대화를 저장 -> 대화 내용이 길어질수록 저장\n",
    "- text completion에 사용함\n",
    "- 5 종류의 메모리가 있는데 이는 같은 API를 공유함 (save_context,load_memory_variables)\n",
    "- return_messages 는 chat을 사용할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"inpuy\":\"Hi\"},{\"output\":\"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"inpuy\":\"Hi\"},{\"output\":\"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer Window Memory\n",
    "- 대화의 특정 부분만 저장하는 방법 (특정 범위)\n",
    "- 최근 대화만 집중한다는 단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages = True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"output\":output})\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5,5)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary Memory\n",
    "- llm을 사용하는 memory\n",
    "- conversation을 요약해서 저장\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Hi, my name is Donghyun Ann, I live in South Korea.\",\"Wow that is so cool\")\n",
    "add_message(\"South Korea is so pretty.\",\"I wish I could go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary Buffer Memory\n",
    "- 메모리에 보내온 메시지의 수를 저장\n",
    "- limit에 도달하면 오래된 메시지는 요약해서 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit = 100,\n",
    "    return_messages= True,\n",
    "    )\n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi, my name is Donghyun Ann, I live in South Korea.\",\"Wow that is so cool\")\n",
    "add_message(\"South Korea is so pretty.\",\"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Korea form Argentina.\",\"I don't know! Super far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Kknowledge Graph Memory\n",
    "- Knowledge Graph를 만들어냄 = 가장 중요한 내용만 뽑아낸 요약본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages= True,\n",
    "    )\n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "# def get_history():\n",
    "#     return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi, my name is Donghyun. I live in South Korea.\",\"Wow that is so cool\")\n",
    "# add_message(\"South Korea is so pretty.\",\"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"input\":\"Who is Donghyun?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Donghyun likes Kimchi\",\"Wow that is so cool\")\n",
    "memory.load_memory_variables({\"input\":\"Who is Donghyun?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory on LLMChain\n",
    "- template 안에 memory를 집어넣는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = PromptTemplate.from_template(template),\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "chain.predict(question = \"My name is Danny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question = \"I live in Seongnam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question = \"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate,MessagesPlaceholder\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "chain.predict(question = \"My name is Danny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question = \"I live in Seongnam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question = \"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key=\"history\", #default\n",
    "    return_messages= True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history = load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result= chain.invoke(\n",
    "        {\n",
    "        \"question\": question\n",
    "        }\n",
    "    )\n",
    "    memory.save_context(\n",
    "        {\"input\":question},\n",
    "        {\"output\":result.content},\n",
    "        )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"My name is Donghyun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Retrieval](https://python.langchain.com/docs/modules/data_connection/)\n",
    "### Loader: 다양한 로더가 있고 3rd party 도 있음 -> slack ,telegram, twitter 등\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파워포인트 html pdf 이미지 등등 다 가능함\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# 에러가 날 경우  설치가 필요함\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load()\n",
    "splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform: 문서 분할\n",
    "- 임베딩, 저장을 위해 필요한 파일의 부분들을 전달해줘야 함\n",
    "- Text Splitter\n",
    "- [openai tokenizer](https://platform.openai.com/tokenizer)와 같이 토크나이저를 splitter로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 작은 단위로 자르는 방법\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(  \n",
    "    chunk_size = 600,\n",
    "    chunk_overlap =100,   # 문장 분할할때 앞부분 조금 가져옴\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "len(loader.load_and_split(text_splitter=splitter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 다른 splitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(  \n",
    "    separator =  \"\\n\",   # 특정 문자열을 기준으로 자르는것\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap =100,   # 문장 분할할때 앞부분 조금 가져옴\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "len(loader.load_and_split(text_splitter=splitter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(  \n",
    "    separator =  \"\\n\",   # 특정 문자열을 기준으로 자르는것\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap =100,   # 문장 분할할때 앞부분 조금 가져옴\n",
    "    length_function = len,  # 길이를 측정하는 방식을 함수로 정의할 수 있음\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "len(loader.load_and_split(text_splitter=splitter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken 사용\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(  \n",
    "    separator =  \"\\n\",   # 특정 문자열을 기준으로 자르는것\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,   # 문장 분할할때 앞부분 조금 가져옴\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "- Text -> numbers (vector)\n",
    "- [임베딩 확인](https://turbomaze.github.io/word2vecjson/)\n",
    "- [LLM 작동원리](https://youtu.be/2eWuYf-aZE4?si=Kl1QF8_j-f683V3c)\n",
    "- embedding을 저장하고 캐싱해서 사용하는게 효율적임, 매번 embedding을 하는 것은 효율적이지 않음\n",
    "- 클라우드 환경이 아닌 로컬에서 사용하는 것을 Chroma를 Store로 사용할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "vector = embedder.embed_query(\"Hi\")\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedder.embed_documents([\n",
    "    \"Hi\",\n",
    "    \"how\",\n",
    "    \"are\",\n",
    "    \"you longer sentences\"])\n",
    "len(vector),len(vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs,cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"where does winston live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Document Chain](https://python.langchain.com/docs/modules/chains/document/)\n",
    "- stuff: 관련된 doc을 한번에 입력해서 질문\n",
    "- refine: 각각의 doc에 대해 질문하면서 질문을 개선\n",
    "- map reduce: doc에 대해 각각 요약 후 llm에 전달\n",
    "- map rerank: 각 doc에 대해 질문 후 대답에 대한 점수 -> 가장 높은 점수를 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Winston Smith lives in Victory Mansions.'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stuff\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm =ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs,cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"WHere does Winston live?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the additional context provided, Winston Smith lives in Victory Mansions, an apartment complex, which is located within the dystopian society depicted in George Orwell\\'s novel, \"1984.\"'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refine\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm =ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs,cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever = vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"WHere does Winston live?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is described as having glass doors at the entrance, which allow gritty dust to enter. The hallway of Victory Mansions has a smell of boiled cabbage and old rag mats. There is a large colored poster on one end of the hallway, depicting the face of a man in his forties with a black mustache. The building has seven floors, and the flat where Winston lives is on the seventh floor. The flat is accessed by stairs since the lift is rarely working. The building is not well-maintained, with rotting houses and patched windows. From the roof of Victory Mansions, one can see the other three buildings that house the Ministries of Truth, Peace, and Love.')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL을 사용한 stuff chain (RetrievalQA 사용X)\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm =ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs,cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "# RunnablePassthrough : 입력한 값 집어넣기\n",
    "chain = {\"context\":retriver,\"question\":RunnablePassthrough()}|prompt | llm\n",
    "chain.invoke(\"Describe Vioctory Mansions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In the given portion of the text, there is no mention of any ministries.')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map Reduce Chain\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\", chunk_size=600, chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "        Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim.\n",
    "        -------\n",
    "        {context}\n",
    "        \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# def map_docs(inputs):\n",
    "#     print(inputs)\n",
    "#     documents = inputs['documents']\n",
    "#     question = input[\"question\"]\n",
    "#     results = []\n",
    "#     for document in documents:\n",
    "#         result = map_doc_chain.invoke(\n",
    "#             {\"context\": document.page_content,\"question\": question}\n",
    "#         )\n",
    "#         results.append(result)\n",
    "#     results = \"\\n\\n\".join(results)\n",
    "#     return results\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "        Given the following extracted parts of a long document and a question, create a final answer. \n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "        ------\n",
    "        {context}\n",
    "        \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Streamlit 사용](https://docs.streamlit.io/get-started/tutorials/create-an-app)\n",
    "- streamlit이 설치되어있는 환경을 활성화\n",
    "- 명령어 입력 ```streamlit run pythonfile```\n",
    "- local host 접속 후 실행 코드 내 수정하면서 변경사항 실시간으로 확인\n",
    "- ```streamlit.write``` 라는 강력한 함숮도 있지만 그냥 객체를 쓰는걸로 바로 같은 효과를 볼 수 있음\n",
    "```python\n",
    "# 같은 결과\n",
    "streamlit.write([1,2,3])\n",
    "[1,2,3]\n",
    "```\n",
    "- 페이지 내 옵션이 재설정 될 때마다 전체가 새롭게 활성화됨 (flutter, Reactjs와는 다름)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import datetime\n",
    "\n",
    "# 제목\n",
    "st.title(\"Hello world!\")\n",
    "# 부제목\n",
    "st.subheader(\"welcome to streamlit!\")\n",
    "\n",
    "# 사이드바\n",
    "with st.sidebar:\n",
    "    st.title(\"Sidebar Title\")\n",
    "    st.text_input(\"XXX\")\n",
    "# 탭\n",
    "tab_one, tab_two, tab_three = st.tabs([\"A\",\"B\",\"C\"])\n",
    "with tab_one:\n",
    "    st.write(\"a\")\n",
    "with tab_two:\n",
    "    st.write(\"b\")\n",
    "with tab_three:\n",
    "    st.write(\"c\")\n",
    "\n",
    "\n",
    "# 마크다운 입력\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    #### I love it!!!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# streamlit.write 실습\n",
    "l = [1, 2, 3, 4]\n",
    "st.write(l)\n",
    "l\n",
    "d = {\"x\": 1}\n",
    "st.write(d)\n",
    "d\n",
    "p = PromptTemplate.from_template(\"xxxx\")\n",
    "st.write(p)\n",
    "p\n",
    "today = datetime.today().strftime(\"%H:%M:%S\")\n",
    "st.write(today)\n",
    "\n",
    "\n",
    "# 선택 박스\n",
    "st.selectbox(\"Choose your model\", (\"GPT-3\", \"GPT-4\"))\n",
    "\n",
    "# text_input\n",
    "name = st.text_input(\"What is your name?\")\n",
    "name\n",
    "\n",
    "# slidebar\n",
    "value = st.slider(\"temperature\",min_value = 0.1, max_value = 1.0)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화창 만들기 예시 (그냥 보여주기)\n",
    "with st.chat_message(\"human\"):\n",
    "    st.write(\"Hello\")\n",
    "\n",
    "with st.chat_message(\"ai\"):\n",
    "    st.write(\"How are you\")\n",
    "\n",
    "with st.status(\"Embedding file...\",expanded=True) as status:\n",
    "    time.sleep(3)\n",
    "    st.write(\"Getting the file\")\n",
    "    time.sleep(3)\n",
    "    st.write(\"Embedding the file\")\n",
    "    time.sleep(3)\n",
    "    st.write(\"Caching the file\")\n",
    "    status.update(label=\"Error\",state = \"error\")\n",
    "\n",
    "st.chat_input(\"Send a message to the AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the memory\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "\n",
    "# Send Message Function\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.write(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "# display messages (history)\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    send_message(message[\"message\"], message[\"role\"], save=False)\n",
    "\n",
    "# 메시지 입력창\n",
    "message = st.chat_input(\"Send a message to the AI\")\n",
    "\n",
    "# 메시지 입력\n",
    "if message:\n",
    "    send_message(message, \"human\")\n",
    "    time.sleep(2)\n",
    "    send_message(f\"You said: {message}\", \"ai\")\n",
    "\n",
    "# 사이드바 로그\n",
    "with st.sidebar:\n",
    "    st.write(st.session_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
